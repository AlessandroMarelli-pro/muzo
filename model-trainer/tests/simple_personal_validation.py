"""
Simple Personal Music Files Validation

Quick validation to test the algorithm on personal music files.
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd

# Add src directory to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

from feature_extraction import AudioFeatureExtractor
from model_training import MusicGenreClassifier


def validate_personal_music_performance():
    """Validate that the algorithm works on personal music files."""
    print("=" * 60)
    print("SIMPLE PERSONAL MUSIC VALIDATION")
    print("=" * 60)

    # Paths
    model_path = Path(__file__).parent.parent / "models" / "personal-genres-v1.0.pkl"
    personal_music_dir = Path("/Users/alessandro/Music/tidal/Tracks")

    # Check files exist
    if not model_path.exists():
        print(f"‚ùå Model file not found: {model_path}")
        return False

    if not personal_music_dir.exists():
        print(f"‚ùå Personal music directory not found: {personal_music_dir}")
        return False

    print(f"‚úÖ Model file found: {model_path}")
    print(f"‚úÖ Personal music directory found: {personal_music_dir}")

    # Load model
    classifier = MusicGenreClassifier()
    classifier.load_model(model_path)
    print(f"‚úÖ Model loaded successfully")
    print(f"   - Model name: {classifier.model_name}")
    print(f"   - Classes: {len(classifier.classes_)}")
    print(f"   - Features: {len(classifier.feature_columns)}")

    # Initialize feature extractor
    feature_extractor = AudioFeatureExtractor()
    print(f"‚úÖ Feature extractor initialized")

    # Get sample of music files (limit to 5 for testing)
    music_files = list(personal_music_dir.glob("*.flac"))[:5]
    print(f"‚úÖ Found {len(music_files)} music files to test")

    results = []
    successful_extractions = 0

    print(f"\nüéµ Testing on {len(music_files)} personal music files:")

    for i, file_path in enumerate(music_files):
        try:
            print(f"   {i + 1:2d}. Processing: {file_path.name}")

            # Extract features
            features = feature_extractor.extract_all_features(str(file_path))

            if features is not None:
                # Create a simple feature vector that matches the model's expectations
                # We'll use a subset of features that are likely to be common
                feature_vector = []

                # Add basic features that should be available
                basic_features = [
                    "length",
                    "chroma_stft_mean",
                    "chroma_stft_var",
                    "rms_mean",
                    "rms_var",
                    "spectral_centroid_mean",
                    "spectral_centroid_var",
                    "spectral_bandwidth_mean",
                    "spectral_bandwidth_var",
                    "rolloff_mean",
                    "rolloff_var",
                    "zero_crossing_rate_mean",
                    "zero_crossing_rate_var",
                    "harmony_mean",
                    "harmony_var",
                    "perceptr_mean",
                    "perceptr_var",
                ]

                for feature in basic_features:
                    if feature in features:
                        feature_vector.append(features[feature])
                    else:
                        feature_vector.append(0.0)  # Default value for missing features

                # Add MFCC features (first 13)
                for i in range(1, 14):
                    mean_key = f"mfcc{i}_mean"
                    var_key = f"mfcc{i}_var"
                    if mean_key in features:
                        feature_vector.append(features[mean_key])
                    else:
                        feature_vector.append(0.0)
                    if var_key in features:
                        feature_vector.append(features[var_key])
                    else:
                        feature_vector.append(0.0)

                # Pad or truncate to match expected feature count (58)
                while len(feature_vector) < 58:
                    feature_vector.append(0.0)
                feature_vector = feature_vector[:58]

                # Convert to numpy array and reshape
                X = np.array(feature_vector).reshape(1, -1)

                # Scale features
                X_scaled = classifier.scaler.transform(X)

                # Make prediction
                prediction = classifier.predict(X_scaled)

                results.append(
                    {
                        "filename": file_path.name,
                        "predicted_genre": prediction["primary_genre"],
                        "confidence": prediction["confidence"],
                        "top_alternatives": prediction["alternative_genres"][:3],
                    }
                )

                successful_extractions += 1
                print(
                    f"       ‚Üí Predicted: {prediction['primary_genre']} (confidence: {prediction['confidence']:.3f})"
                )
            else:
                print(f"       ‚Üí Failed: Could not extract features")

        except Exception as e:
            print(f"       ‚Üí Error: {str(e)}")

    print(f"\nüìä Results:")
    print(f"   - Total files processed: {len(music_files)}")
    print(f"   - Successful extractions: {successful_extractions}")
    print(f"   - Success rate: {successful_extractions / len(music_files) * 100:.1f}%")

    if successful_extractions > 0:
        print(f"\nüéµ Sample Predictions:")
        for i, result in enumerate(results[:3]):
            print(f"   {i + 1}. {result['filename']}")
            print(f"      Genre: {result['predicted_genre']}")
            print(f"      Confidence: {result['confidence']:.3f}")
            print(
                f"      Alternatives: {', '.join([alt['genre'] for alt in result['top_alternatives']])}"
            )
            print()

        # Analyze genre distribution
        genre_counts = {}
        confidence_scores = []

        for result in results:
            genre = result["predicted_genre"]
            confidence = result["confidence"]

            genre_counts[genre] = genre_counts.get(genre, 0) + 1
            confidence_scores.append(confidence)

        print(f"üìà Genre Distribution:")
        for genre, count in sorted(
            genre_counts.items(), key=lambda x: x[1], reverse=True
        ):
            percentage = (count / len(results)) * 100
            print(f"   {genre}: {count} files ({percentage:.1f}%)")

        avg_confidence = np.mean(confidence_scores)
        print(f"\nüéØ Average Confidence: {avg_confidence:.3f}")

        print(f"\n‚úÖ SUCCESS: Algorithm works on personal music files!")
        print(f"   - Successfully processed {successful_extractions} files")
        print(f"   - Average confidence: {avg_confidence:.3f}")
        print(f"   - Genre diversity: {len(genre_counts)} different genres")

        return True
    else:
        print(f"\n‚ùå FAILURE: Could not process any personal music files")
        return False


if __name__ == "__main__":
    success = validate_personal_music_performance()
    sys.exit(0 if success else 1)
